# This example demonstrates the ability to pass artifacts
# from one step to the next.
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: dtw-compare-
spec:
  entrypoint: read-folders
  parallelism: 6
  templates:
  - name: read-folders
    steps:
    - - name: read-input 
        template: read-input-template
    - - name: dtw-compare-
        template: dtw-compare-wat-template
        arguments:
          parameters:
          - name: folder
            value: "{{item.folder}}"
          - name: filter
            value: "{{item.filter}}"
        withParam: "{{steps.read-input.outputs.parameters.programs}}"
        continueOn:
          failed: true
    - - name: reduce
        template: merge
  - name: read-input-template
    inputs: 
      artifacts:
        - name: "crow_out"
          path: /input2.json
          s3:
            bucket: my-bucket
            key:  /input2.json
    script:
      image: ubuntu:latest
      command: ['bash']
      source: |
        cat /input2.json

    outputs:
      parameters:
        - name: programs
          valueFrom: 
            path: "/input2.json"
  - name: dtw-compare-wat-template
    inputs: 
      parameters:
      - name: folder
      - name: filter
      artifacts:
        - name: "crow_out"
          path: /crow_out
          s3:
            bucket: my-bucket
            key:  /crow_out/{{inputs.parameters.folder}}
        - name: "payload"
          path: /wat_dtw_payload_template.json
          s3:
            bucket: my-bucket
            key:  /wat_dtw_payload_template.json
      
    outputs:
      artifacts:
      - name: strac
        path: /WORKDIR/out/align.result.json
        archive:
          # prevent compression
          none: { }
        s3:
          key: /results_strac_{{workflow.name}}/{{inputs.parameters.folder}}.json
      
      - name: strac_files
        path: /WORKDIR/out
        s3:
          key: /results_strac_all_{{workflow.name}}/{{inputs.parameters.folder}}.tgz

    script:
      image: jacarte/strac:latest3
      command: ['bash']
      source: |

        cp /wat_dtw_payload_template.json /WORKDIR/default.json
        ls -R /crow_out
        mkdir /INPUTS
        cp /crow_out/crow_out/{{inputs.parameters.folder}}/{{inputs.parameters.filter}} /INPUTS 
        ls -R /WORKDIR

        bash /WORKDIR/launch.sh /INPUTS
      resources:
          requests:
            memory: 2G
            cpu: 2
          limits:
            memory: 5G
            cpu: 4

  - name: merge
    inputs:
      artifacts:
        - name: results
          path: /mnt/in
          s3:
            key: "/results_strac_{{workflow.name}}"
    script:
      image: python:alpine3.6
      command: 
          - python
      source: |
        import json
        import os
        import sys
        total = 0
        os.mkdir("/mnt/out")
        OVERALL = {}
        for f in os.listdir("/mnt/in/results_strac_{{workflow.name}}"):
          print(f)
          result = json.loads(open(f"/mnt/in/results_strac_{{workflow.name}}/{f}", "r").read())
          OVERALL[f]=result
        with open("/mnt/out/total.json" , "w") as f:
          json.dump(OVERALL, f)
    outputs:
      artifacts:
        - name: overall
          path: /mnt/out/total.json
          archive:
            none: { }
          s3:
            key: "strac_{{workflow.name}}/alignments.json"
